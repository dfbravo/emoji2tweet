{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emoji2tweet_evaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOUg1Ttjjeh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrQWnF6EOuug",
        "colab_type": "text"
      },
      "source": [
        "# Load Data from GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7W5uBYEi00T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8269729e-0ca8-412b-e32e-e96e00cab7dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-CE1aIORLSF",
        "colab_type": "text"
      },
      "source": [
        "## Read and Prepare Data for Model Input\n",
        "I read the data from GDrive and add placeholder tokens to denote the start and end of a sequence. These are added to every tweet in the dataset.\n",
        "\n",
        "The placeholders are required for the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JBD9AC7O5x9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "cb0c9c0e-da49-48d9-8ca8-b4f2a3467a26"
      },
      "source": [
        "full_test_df = pd.read_csv('/content/drive/My Drive/Capstone/testing_tweets.csv', index_col=0)\n",
        "\n",
        "# evaluate on training dataset to determine if the model is even learning anything\n",
        "# full_test_df = pd.read_csv('/content/drive/My Drive/Capstone/training_tweets.csv', index_col=0)\n",
        "\n",
        "# Add placeholder to start and end of tweet\n",
        "full_test_df['tweet_text'] = full_test_df['tweet_text'].apply(lambda x: ' '.join(['startseq', x, 'endseq']))\n",
        "#update n_tokens\n",
        "full_test_df['n_tokens'] = full_test_df['n_tokens'] + 2\n",
        "full_test_df"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emojis</th>\n",
              "      <th>emojis_unq</th>\n",
              "      <th>n_tokens</th>\n",
              "      <th>n_emojis</th>\n",
              "      <th>n_emojis_unq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>602356</th>\n",
              "      <td>startseq üåü Out now üåü How to identify &amp; explain...</td>\n",
              "      <td>üåü üåü üåß ‚òÄ</td>\n",
              "      <td>‚òÄ üåü üåß</td>\n",
              "      <td>23</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>563134</th>\n",
              "      <td>startseq Another sleeper üò° Esper Orders Nation...</td>\n",
              "      <td>üò°</td>\n",
              "      <td>üò°</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191830</th>\n",
              "      <td>startseq gratitude bloodline üñ§ u next endseq</td>\n",
              "      <td>üñ§</td>\n",
              "      <td>üñ§</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443691</th>\n",
              "      <td>startseq Cross the map üó∫ endseq</td>\n",
              "      <td>üó∫</td>\n",
              "      <td>üó∫</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359484</th>\n",
              "      <td>startseq Always remember to NEVER QUIT üí™ üè† @ C...</td>\n",
              "      <td>üí™ üè†</td>\n",
              "      <td>üè† üí™</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342845</th>\n",
              "      <td>startseq thank you lexo ‚ù§ Ô∏è ‚ù§ Ô∏è endseq</td>\n",
              "      <td>‚ù§ ‚ù§</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>640052</th>\n",
              "      <td>startseq Oh yes naman syempre üëç endseq</td>\n",
              "      <td>üëç</td>\n",
              "      <td>üëç</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>583289</th>\n",
              "      <td>startseq Thank you ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è endseq</td>\n",
              "      <td>‚ù§ ‚ù§ ‚ù§</td>\n",
              "      <td>‚ù§</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496851</th>\n",
              "      <td>startseq üòñ üòñ üòñ them some fuckin talons lady Id...</td>\n",
              "      <td>üòñ üòñ üòñ</td>\n",
              "      <td>üòñ</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248191</th>\n",
              "      <td>startseq Looking for a pirate ship ( lil boat ...</td>\n",
              "      <td>üö£‚Äç‚ôÇÔ∏è</td>\n",
              "      <td>üö£‚Äç‚ôÇÔ∏è</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36398 rows √ó 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet_text  ... n_emojis_unq\n",
              "602356  startseq üåü Out now üåü How to identify & explain...  ...            3\n",
              "563134  startseq Another sleeper üò° Esper Orders Nation...  ...            1\n",
              "191830       startseq gratitude bloodline üñ§ u next endseq  ...            1\n",
              "443691                    startseq Cross the map üó∫ endseq  ...            1\n",
              "359484  startseq Always remember to NEVER QUIT üí™ üè† @ C...  ...            2\n",
              "...                                                   ...  ...          ...\n",
              "342845             startseq thank you lexo ‚ù§ Ô∏è ‚ù§ Ô∏è endseq  ...            1\n",
              "640052             startseq Oh yes naman syempre üëç endseq  ...            1\n",
              "583289              startseq Thank you ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è endseq  ...            1\n",
              "496851  startseq üòñ üòñ üòñ them some fuckin talons lady Id...  ...            1\n",
              "248191  startseq Looking for a pirate ship ( lil boat ...  ...            1\n",
              "\n",
              "[36398 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPjYYIgpZCq6",
        "colab_type": "text"
      },
      "source": [
        "# Load word2vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukzghkSXkjwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "d9e354c2-c486-476c-dc24-3c23f7756aaa"
      },
      "source": [
        "# w2v_model loaded outside of the function to prevent loading the model on every function call\n",
        "w2v_model = Word2Vec.load('/content/drive/My Drive/Capstone/w2v.model')\n",
        "def vectorize_emojis(emoji_str):\n",
        "    emojis = emoji_str.split(' ')\n",
        "    vec_sum = np.zeros(300)\n",
        "    for emj in emojis:\n",
        "        try:\n",
        "            vec = w2v_model.wv[emj]\n",
        "            vec_sum += vec\n",
        "        except KeyError:\n",
        "            pass\n",
        "            #print(f'Emoji not in w2v: {emj}')\n",
        "            #ignoring emojis not in the list\n",
        "    return vec_sum"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LPPrDvGZIrg",
        "colab_type": "text"
      },
      "source": [
        "## Defining Some Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJoSNsjqQHp_",
        "colab_type": "text"
      },
      "source": [
        "I limit the number of tweets used for testing in order to reduce execution time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFA_bwoQPBGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# token limit of tweets to be generated\n",
        "n_max_tokens = 32\n",
        "\n",
        "# number of tweets used for testing\n",
        "n_samples = 1000\n",
        "# Arbitrary choice, but doesn't matter since the model is not training on testing data \n",
        "# The validation set is the first n_sample tweets from the test set\n",
        "valid_df = full_test_df.head(n_samples)\n",
        "# The testing set is the last n_sample tweets from the test set\n",
        "test_df = full_test_df.tail(n_samples)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bh1The6SFdj",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Tokenizer\n",
        "\n",
        "Here I load the tokenizer that was created when training the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaB_et-wSE0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = pickle.load(open('/content/drive/My Drive/Capstone/emoji2tweet_tokenizer_n5000.pkl', 'rb'))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IUwYIxNQ0-E",
        "colab_type": "text"
      },
      "source": [
        "## Generating Tweets\n",
        "\n",
        "Here I create a helper function that will be used to generate tweets. The tweets are generated token-by-token. The token with the highest probability is chosen as the next token.\n",
        "\n",
        "I want to explore different methods of choosing the next token. To introduce variability in the tweets generated, I could choose 1 of the top 5 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8l43IypkWvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is used to generate a tweet token-by-token\n",
        "# function taken and adapted from: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "def generate_tweet(tweetmoji_model, tokenizer, n_max_tokens, in_emojis, in_text = 'startseq'):\n",
        "  # Create feature vector using word2vec\n",
        "  emoji_vec = vectorize_emojis(in_emojis)\n",
        "  # Convert the currently generate tweet into a sequence \n",
        "  seq = tokenizer.texts_to_sequences([in_text])[0]\n",
        "  # Store the index of 'enqseq' to use as stopping condition\n",
        "  endseq = tokenizer.texts_to_sequences(['endseq'])[0]\n",
        "  # Generate token-by-token up to the endseq or the token limit\n",
        "  for i in range(n_max_tokens):\n",
        "    # input sequences must be padded as input for the model\n",
        "    seq_pad = pad_sequences([seq],maxlen=n_max_tokens)\n",
        "    # The token predicted is the token with the highst probability\n",
        "    y_pred = np.argmax(tweetmoji_model.predict([[emoji_vec], seq_pad]))\n",
        "    seq.append(y_pred)\n",
        "    if y_pred == endseq[0]:\n",
        "      break\n",
        "  tweet = tokenizer.sequences_to_texts([seq])[0]\n",
        "  return tweet"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rkmnfJvSNH-",
        "colab_type": "text"
      },
      "source": [
        "## Calculating BLEU Score\n",
        "\n",
        "Here I create a helper function that is used to calculate the BLEU score for a generated tweet. \n",
        "\n",
        "From the test set I grab a tweet, extract its emojis and use it as input for the model. I compare the generated tweet with the original tweet. The BLEU scores are calculated for 1- to 4-grams.\n",
        "\n",
        "Ideally I would compare the generated tweet with a set of reference tweets that are representative of the concept/idea the emojis are meant to represent. This remains an area I want to explore further. Can I cluster tweets based on their similarity scores and used those as reference tweets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOvjoacnjjmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bleu_eval(model, df, tokenizer, n_max_tokens):\n",
        "  # lists that contain the generated and reference tweets\n",
        "  actual, predicted = [], []\n",
        "  bleu_scores = []\n",
        "  for _, row in df.iterrows():\n",
        "    # Curently comparing to the tweet from the testing set\n",
        "    reference_tweets = [row['tweet_text']]\n",
        "    # generate a full tweet\n",
        "    gen_tweet = generate_tweet(model, tokenizer, n_max_tokens, row['emojis'])\n",
        "    # add the tweets to the appropriate lists\n",
        "    actual.append(reference_tweets)\n",
        "    predicted.append(gen_tweet.split())\n",
        "  # Compute BLEU score\n",
        "  bleu1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
        "  bleu2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
        "  bleu3 = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n",
        "  bleu4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  return [bleu1, bleu2, bleu3, bleu4]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmcBqO8YPcdP",
        "colab_type": "text"
      },
      "source": [
        "# Validating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBuG3KtZPg6g",
        "colab_type": "text"
      },
      "source": [
        "First I define where the models are located, and which models I want to test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwweAL2ePn8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The model names follow a standard naming scheme\n",
        "model_names = [f'tweetmoji-epoch{n}.h5' for n in range(1,11)]\n",
        "\n",
        "# The directory where the models are found\n",
        "#model_dir = '/content/drive/My Drive/Capstone/model_output/n_samples5000_n_epoch10_add'\n",
        "model_dir = '/content/drive/My Drive/Capstone/model_output/n_samples5000_n_epoch10_conc'\n",
        "#model_dir = '/content/drive/My Drive/Capstone/model_output/n_samples5000_n_epoch10_unq_add'\n",
        "#model_dir = '/content/drive/My Drive/Capstone/model_output/n_samples5000_n_epoch10_unq_conc'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MA5w8y3jphI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "2dbc8ff3-638f-4336-ba7a-f0b1c532c920"
      },
      "source": [
        "input_df = test_df[['tweet_text', 'emojis']]\n",
        "\n",
        "bleu_scores = []\n",
        "for n, model_name in enumerate(model_names):\n",
        "  epoch = n+1\n",
        "  model_filename = os.path.join(model_dir, model_name)\n",
        "  model = load_model(model_filename)\n",
        "  bleu_scores.append(bleu_eval(model, input_df, tokenizer, n_max_tokens))\n",
        "\n",
        "bleu_df = pd.DataFrame(bleu_scores, columns=['bleu-1', 'bleu-2', 'bleu-3', 'bleu-4'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNbMXwlXjD6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bleu_df.to_csv(f'{model_dir}/bleu_scores_test.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmldeETza2J9",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating the Testing Set\n",
        "\n",
        "After analyzing the plots for the BLEU Scores, which were created in a separate notebook. The model that had the best results used concatenation for merging, and used the verbatim emojis extracted from the tweet. The best results were found at epoch 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnmZXBOFdJUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "3286f1ae-eb56-4f8d-80ba-fa334077d706"
      },
      "source": [
        "# Grab the right model\n",
        "model_dir = '/content/drive/My Drive/Capstone/model_output/n_samples5000_n_epoch10_conc'\n",
        "model_name = 'tweetmoji-epoch2.h5' # grabbing model at 2nd epoch\n",
        "model_filepath = os.path.join(model_dir, model_name)\n",
        "# load the model\n",
        "model = load_model(model_filepath)\n",
        "bleu_scores = []\n",
        "bleu_scores.append(bleu_eval(model, input_df, tokenizer, n_max_tokens))\n",
        "bleu_scores"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.018914905099723208,\n",
              "  0.07179588532972878,\n",
              "  0.12241022907824407,\n",
              "  0.1398773028468326]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvRGhGSlgoZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "0fb46329-3151-43fc-c8d0-11d0a06d0001"
      },
      "source": [
        "bleu_scores_df = pd.DataFrame(bleu_scores[0], index=['bleu-1', 'bleu-2', 'bleu-3', 'bleu-4'], columns=['score'])\n",
        "bleu_scores_df"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bleu-1</th>\n",
              "      <td>0.018915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bleu-2</th>\n",
              "      <td>0.071796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bleu-3</th>\n",
              "      <td>0.122410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bleu-4</th>\n",
              "      <td>0.139877</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           score\n",
              "bleu-1  0.018915\n",
              "bleu-2  0.071796\n",
              "bleu-3  0.122410\n",
              "bleu-4  0.139877"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MwpBaHFTvdR",
        "colab_type": "text"
      },
      "source": [
        "## Generating Sample Tweets\n",
        "\n",
        "To generate sample tweets I first load a specific model. I then create a helper function to remove the start and end sequences tokens. Finally I use the previous `generte_tweet` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQyDviZXUC0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a specific Model\n",
        "model_filename = '/content/drive/My Drive/Capstone/model_output/20200624_2257/tweetmoji-epoch6.h5'\n",
        "model = load_model(model_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVmnnK8qjLrJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ccbc8c4a-8537-45c9-f299-390aa802e737"
      },
      "source": [
        "# Helper function to remove the start and end sequence tokens\n",
        "def remove_seq_tokens(tweet_str):\n",
        "  tweet_tokens = tweet_str.split(' ')\n",
        "  tweet_tokens = tweet_tokens[1:-1]\n",
        "  return ' '.join(tweet_tokens)\n",
        "\n",
        "emoji_inputs = ['üíó üéÑ üë™', 'üéÅ üéÇ üéà', 'üòÇ üò≠', '‚ù§ üéÅ', 'üèÄ üëë üî•', 'üòÖ ‚ù§ ü§ó']\n",
        "\n",
        "generated_tweets = []\n",
        "for emojis in emoji_inputs:\n",
        "  gen_tweet = generate_tweet(model, tokenizer, n_max_tokens, emojis)\n",
        "  stripped_tweet = remove_seq_tokens(gen_tweet)\n",
        "  #generated_tweets.append(gen_tweet)\n",
        "  print (f'{emojis} => \"{remove_seq_tokens(gen_tweet)}\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "üíó üéÑ üë™ => \"i love you üíñ\"\n",
            "üéÅ üéÇ üéà => \"happy birthday üéÇ üéâ üéâ\"\n",
            "üòÇ üò≠ => \"i need a <unk> üòÇ üò≠\"\n",
            "‚ù§ üéÅ => \"i need to be <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\"\n",
            "üèÄ üëë üî• => \"<unk> üî• üî•\"\n",
            "üòÖ ‚ù§ ü§ó => \"i need to be <unk> üòò\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}