{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emoji2tweet_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5eMUGBJ24zC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vIbLZnETJJr",
        "colab_type": "text"
      },
      "source": [
        "# Load Data from GDrive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75SPY3bcTNhU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "31182229-46e2-49dc-ac54-80195fb327fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpkUod6yaJNe",
        "colab_type": "text"
      },
      "source": [
        "# Read and Prepare Data for Model Input\n",
        "I read the data from GDrive and add placeholder tokens to denote the start and end of a sequence. These are added to every tweet in the dataset.\n",
        "\n",
        "The placeholders are required for the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2bM-QOi7_vH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "ee8f89e1-3278-4d78-9f22-3dd34db1ed5f"
      },
      "source": [
        "full_train_df = pd.read_csv('/content/drive/My Drive/Capstone/training_tweets.csv', index_col=0)\n",
        "# Add placeholder to start and end of tweet\n",
        "full_train_df['tweet_text'] = full_train_df['tweet_text'].apply(lambda x: ' '.join(['startseq', x, 'endseq']))\n",
        "#update n_tokens\n",
        "full_train_df['n_tokens'] = full_train_df['n_tokens'] + 2\n",
        "full_train_df"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emojis</th>\n",
              "      <th>emojis_unq</th>\n",
              "      <th>n_tokens</th>\n",
              "      <th>n_emojis</th>\n",
              "      <th>n_emojis_unq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26760</th>\n",
              "      <td>startseq Smh ü§¶üèΩ‚Äç‚ôÇÔ∏è things have got to change e...</td>\n",
              "      <td>ü§¶üèΩ‚Äç‚ôÇÔ∏è</td>\n",
              "      <td>ü§¶üèΩ‚Äç‚ôÇÔ∏è</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150342</th>\n",
              "      <td>startseq He‚Äôll probably just take it in stride...</td>\n",
              "      <td>ü§£</td>\n",
              "      <td>ü§£</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>616357</th>\n",
              "      <td>startseq God Bless America üòû endseq</td>\n",
              "      <td>üòû</td>\n",
              "      <td>üòû</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343728</th>\n",
              "      <td>startseq I don‚Äôt have Venmo ‚òπ Ô∏è I have cashapp...</td>\n",
              "      <td>‚òπ</td>\n",
              "      <td>‚òπ</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66344</th>\n",
              "      <td>startseq A little wip I‚Äôm working on ~ can fin...</td>\n",
              "      <td>üëÄ</td>\n",
              "      <td>üëÄ</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51304</th>\n",
              "      <td>startseq All it takes is ONE google search The...</td>\n",
              "      <td>ü§°</td>\n",
              "      <td>ü§°</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446871</th>\n",
              "      <td>startseq To the nail shop I go ü•∞ endseq</td>\n",
              "      <td>ü•∞</td>\n",
              "      <td>ü•∞</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538154</th>\n",
              "      <td>startseq Thank you for sharing your story It i...</td>\n",
              "      <td>üí™üèΩ</td>\n",
              "      <td>üí™üèΩ</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48438</th>\n",
              "      <td>startseq üö® NEW VIDEOS Alert üö® Had a very üî• üî• üî•...</td>\n",
              "      <td>üö® üö® üî• üî• üî•</td>\n",
              "      <td>üî• üö®</td>\n",
              "      <td>22</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>617416</th>\n",
              "      <td>startseq A message to all our patients from ou...</td>\n",
              "      <td>‚ò∫</td>\n",
              "      <td>‚ò∫</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>84928 rows √ó 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet_text  ... n_emojis_unq\n",
              "26760   startseq Smh ü§¶üèΩ‚Äç‚ôÇÔ∏è things have got to change e...  ...            1\n",
              "150342  startseq He‚Äôll probably just take it in stride...  ...            1\n",
              "616357                startseq God Bless America üòû endseq  ...            1\n",
              "343728  startseq I don‚Äôt have Venmo ‚òπ Ô∏è I have cashapp...  ...            1\n",
              "66344   startseq A little wip I‚Äôm working on ~ can fin...  ...            1\n",
              "...                                                   ...  ...          ...\n",
              "51304   startseq All it takes is ONE google search The...  ...            1\n",
              "446871            startseq To the nail shop I go ü•∞ endseq  ...            1\n",
              "538154  startseq Thank you for sharing your story It i...  ...            1\n",
              "48438   startseq üö® NEW VIDEOS Alert üö® Had a very üî• üî• üî•...  ...            2\n",
              "617416  startseq A message to all our patients from ou...  ...            1\n",
              "\n",
              "[84928 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk5ULKiXGeZF",
        "colab_type": "text"
      },
      "source": [
        "I have to reduce the dataset to reduce the training time. I use the first 5000 tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsEQXwLoGhXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 5000\n",
        "train_df = full_train_df.head(n_samples)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK7V0LhobNXH",
        "colab_type": "text"
      },
      "source": [
        "I need to keep track of the maximum number of tokens in a tweet, and the size of the vocabulary, as they dictate the size of vectors used in the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k_ENxmcPrgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d282dc4c-cada-447f-a637-28ee2f0c6782"
      },
      "source": [
        "# value required to define the model\n",
        "# dictates size of internal vector\n",
        "n_max_tokens = train_df['n_tokens'].max()\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<unk>')\n",
        "tokenizer.fit_on_texts(train_df['tweet_text'])\n",
        "# word_index returns the actual vocab size of the corpus\n",
        "# but uses the max vocab size passed on in num_words\n",
        "# see: https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work\n",
        "# add 1 to account for <unk>\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Maximum number of tokens per tweet: {n_max_tokens}')\n",
        "print(f'Vocabulary size: {vocab_size}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum number of tokens per tweet: 32\n",
            "Vocabulary size: 10036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTe0SkYvj9DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pickle.dump(tokenizer, open('/content/drive/My Drive/Capstone/emoji2tweet_tokenizer_n5000.pkl', 'wb'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM7WAJB3bsxd",
        "colab_type": "text"
      },
      "source": [
        "## Load Word2Vec Model\n",
        "I load the word2vec model that will be used to create the feature vector. Each emoji is converted to a vector using this model. I create a helper function to quickly convert a string of emojis (as it is stored in the dataframe), into the resulting feature vector. The feature vector is the sum of the vector representations of each emoji in the string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edJBjJCm-OM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "24fcea95-8065-4dfe-8e3e-78582c61daea"
      },
      "source": [
        "# w2v_model loaded outside of the function to prevent loading the model on every function call\n",
        "w2v_model = Word2Vec.load('/content/drive/My Drive/Capstone/w2v.model')\n",
        "def vectorize_emojis(emoji_str):\n",
        "    emojis = emoji_str.split(' ')\n",
        "    vec_sum = np.zeros(300)\n",
        "    for emj in emojis:\n",
        "        try:\n",
        "            vec = w2v_model.wv[emj]\n",
        "            vec_sum += vec\n",
        "        except KeyError:\n",
        "            pass\n",
        "            #print(f'Emoji not in w2v: {emj}')\n",
        "            #ignoring emojis not in the list\n",
        "    return vec_sum"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqrP2ZbMcGmr",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Input to the Model\n",
        "The tweet text has to be expanded into partial sequences so that it can be fed into the RNN. The partial sequences are essentially concatenated with the feature vector.\n",
        "\n",
        "The `create_sequences` function will generate this data. Though it is very memory intensive, and I had to implement a data generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDD2Z7OeUB1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function taken and adapted from: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "def create_sequences(df, n_max_tokens, vocab_size):\n",
        "  # initiate empty lists\n",
        "  rnn_seqs, emoji_vecs, y = [], [], []\n",
        "  #I'm iterating over a df, yuck\n",
        "  #just cant think of the alternative atm\n",
        "  for _, row in df.iterrows():\n",
        "    tweet_text = row['tweet_text']\n",
        "    emoji_vec = vectorize_emojis(row['emojis'])\n",
        "    seq = tokenizer.texts_to_sequences([tweet_text])[0]\n",
        "    for i in range(1, len(seq)):\n",
        "      # split into input and output pair\n",
        "      in_seq, out_seq = seq[:i], seq[i]\n",
        "      # pad input sequence\n",
        "      in_seq = pad_sequences([in_seq], maxlen=n_max_tokens)[0]\n",
        "      # encode output sequence\n",
        "      out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "      emoji_vecs.append(emoji_vec)\n",
        "      rnn_seqs.append(in_seq)\n",
        "      y.append(out_seq)\n",
        "  return np.array(emoji_vecs), np.array(rnn_seqs), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6G29x7zVl4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functions taken and adapted from: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "def model_data_generator(df, n_max_tokens, vocab_size):\n",
        "  while 1:\n",
        "    for _, row in df.iterrows():\n",
        "      tweet_text = row['tweet_text']\n",
        "      emoji_str = row['emojis']\n",
        "      emoji_vecs, rnn_seqs, y = create_sequences_gen(tweet_text, emoji_str, n_max_tokens, vocab_size)\n",
        "      yield [[emoji_vecs, rnn_seqs], y]\n",
        "\n",
        "def create_sequences_gen(tweet_text, emoji_str, n_max_tokens, vocab_size):\n",
        "  # initiate empty lists\n",
        "  rnn_seqs, emoji_vecs, y = [], [], []\n",
        "  emoji_vec = vectorize_emojis(emoji_str)\n",
        "  seq = tokenizer.texts_to_sequences([tweet_text])[0]\n",
        "  for i in range(1, len(seq)):\n",
        "    # split into input and output pair\n",
        "    in_seq, out_seq = seq[:i], seq[i]\n",
        "    # pad input sequence\n",
        "    in_seq = pad_sequences([in_seq], maxlen=n_max_tokens)[0]\n",
        "    # encode output sequence\n",
        "    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "    emoji_vecs.append(emoji_vec)\n",
        "    rnn_seqs.append(in_seq)\n",
        "    y.append(out_seq)\n",
        "  return np.array(emoji_vecs), np.array(rnn_seqs), np.array(y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKtBCDscr_I",
        "colab_type": "text"
      },
      "source": [
        "## Defining the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qszCJ5NOkIMx",
        "colab_type": "text"
      },
      "source": [
        "Here I create two functions that define the model. The first function merges the feature vector and the result of the RNN through addition. The second function concatenates along axis=1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp1bOHi5kE_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function taken and adapted from: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "def define_model_add(n_max_tokens, vocab_size):\n",
        "  # word2vec 300dm vector as input\n",
        "  inputs1 = Input(shape=(300,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  # Use dense layer to \n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "  # sequence model\n",
        "  inputs2 = Input(shape=(n_max_tokens,))\n",
        "  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "  # decoder model\n",
        "  decoder1 = add([fe2, se3])\n",
        "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  # summarize model\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e1Mr4KFcdtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function taken and adapted from: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "def define_model_conc(n_max_tokens, vocab_size):\n",
        "  # feature extractor model\n",
        "  inputs1 = Input(shape=(300,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "  # sequence model\n",
        "  inputs2 = Input(shape=(n_max_tokens,))\n",
        "  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "  # decoder model\n",
        "  decoder1 = concatenate(inputs=[fe2, se3], axis=1)\n",
        "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  # summarize model\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdzCT9hObwKF",
        "colab_type": "text"
      },
      "source": [
        "The model is instanttiated here. Initially I saved an empty model in order to have the same starting condition for different test instance, but it was hard to track as I changed my model parameters (token limit, vocab size)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nty2WIvekN9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "6e0b1ba8-1ce1-491f-e5de-1c6678085682"
      },
      "source": [
        "model = define_model_conc(n_max_tokens, vocab_size)\n",
        "# Save an empty model that I can use to \"reset\" my weights\n",
        "#model.save('/content/drive/My Drive/Capstone/tweetmoji_empty_model.h5')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 32)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 300)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 32, 256)      2569216     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 300)          0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 256)      0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          77056       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 256)          525312      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 512)          0           dense_5[0][0]                    \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          131328      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 10036)        2579252     dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,882,164\n",
            "Trainable params: 5,882,164\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W07VB0nRypsL",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ZliUw-4hV8",
        "colab_type": "text"
      },
      "source": [
        "The model is trained over 10 epochs. I create an output folder where the resulting models will be saved. I save the model after every epoch. Additionally, I save the training loss.\n",
        "\n",
        "I trained the model using 2 different inputs, and 2 different methods of merging.\n",
        "\n",
        "For the first set of inputs I used the list of emojis extracted from the tweet. For the second set I used a unique list of eemojis extracted from the tweet.\n",
        "\n",
        "I used the unique list to remove the influence of repeated emojis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOB-f7QHtbYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "8e78fccb-e9f3-45e0-ba89-b5e5bbff8088"
      },
      "source": [
        "n_epochs = 10\n",
        "input_df = train_df[['tweet_text', 'emojis']]\n",
        "\n",
        "# To train on unique emojis uncomment the lines below\n",
        "# input_df = train_df[['tweet_text', 'emojis_unq']]\n",
        "# input_df.columns = ['tweet_text', 'emojis']\n",
        "\n",
        "# Create output folder\n",
        "os.environ['TZ'] = 'America/Toronto'\n",
        "date_str = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
        "# Use the samples and number of epochs to differentiate model outputs\n",
        "# Use the date_str to avoid overwriting existing outputs\n",
        "output_dir_name = f'n_samples{n_samples}_n_epoch{n_epochs}_{date_str}'\n",
        "output_dir = f\"/content/drive/My Drive/Capstone/model_output/{output_dir_name}\"\n",
        "print(f'OUTPUT: {output_dir}')\n",
        "# Create the directory\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Define naming scheme for model output\n",
        "output_filename=os.path.join(output_dir, \"tweetmoji-epoch{epoch}.h5\")\n",
        "# Use ModelCheckpoint to save the weights after each epoch\n",
        "checkpoint = ModelCheckpoint(output_filename, monitor='loss', period=1)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# reset model weights by loading an \"empty\" model\n",
        "# using the same base model to compare results of different \n",
        "# model = load_model('/content/drive/My Drive/Capstone/tweetmoji_empty_model.h5')\n",
        "\n",
        "# Fit the model using the data generator\n",
        "data_gen = model_data_generator(input_df, n_max_tokens, vocab_size)\n",
        "# the model history is saved as training_loss because that's the only value tracked\n",
        "training_loss = model.fit_generator(data_gen, epochs=n_epochs, steps_per_epoch=n_samples, verbose=1, callbacks=callbacks_list)\n",
        "\n",
        "# Save history of the model\n",
        "# This contain training loss per epoch\n",
        "training_loss_df = pd.DataFrame(training_loss.history)\n",
        "training_loss_df.index.name = 'epoch'\n",
        "training_loss_df.to_csv(os.path.join(output_dir, 'training_loss.csv'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OUTPUT: /content/drive/My Drive/Capstone/model_output/n_samples5000_n_epoch10_20200627_2127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 415s 83ms/step - loss: 6.8144\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 415s 83ms/step - loss: 5.8629\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 415s 83ms/step - loss: 5.4480\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 415s 83ms/step - loss: 5.1148\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 418s 84ms/step - loss: 4.8277\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 412s 82ms/step - loss: 4.5770\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 418s 84ms/step - loss: 4.3853\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 427s 85ms/step - loss: 4.1998\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 428s 86ms/step - loss: 4.0797\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 420s 84ms/step - loss: 3.9409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNVvf0xdXzEC",
        "colab_type": "text"
      },
      "source": [
        "## Done Training!\n",
        "\n",
        "Evaluation and generation of tweet is done in  emoji2tweet_evaluation.ipynb\n",
        "\n"
      ]
    }
  ]
}